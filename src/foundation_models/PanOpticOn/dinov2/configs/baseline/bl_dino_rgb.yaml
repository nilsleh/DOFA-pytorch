dino:
  koleo_loss_weight: 0.0
train:
  dataset:
    id: FmowDataset
    split: fmow/metadata_v2/fmow_iwm_onid_train_val_presorted.parquet
    root: ${oc.env:RESOURCE_DIR}/datasets/
    num_sens: 2
    return_rgb: True
    transform:
    - id: ChnSpatialAugmentation
      global_crops_scale: [0.32, 1.0]
      local_crops_number: 4
      local_crops_scale: [0.05, 0.32]
      global_crops_size: [-1,196]
      local_crops_size: [-1,84]
  global_smask_absolute_tuple: 
  local_smask_absolute_tuple: 
  
  batch_size_per_gpu: 165
  num_workers: 12

  use_wandb: True
  saveckp_freq: 1
  seed: 0
  OFFICIAL_EPOCH_LENGTH: 500 # 500
  centering: "centering" # or "sinkhorn_knopp"
  log_every_n_steps: 10
student:
  arch: vit_base
  embed_layer: PatchEmbed
  patch_size: 14
  layerscale: 1.0e-05
  pretrained_weights: /data/panopticon/other_model_ckpts/dinov2/dinov2_vitb14_pretrain_wmodelkey.pth
  ffn_layer: "mlp"
teacher:
  pretrained_weights: /data/panopticon/other_model_ckpts/dinov2/dinov2_vitb14_pretrain_wmodelkey.pth
  momentum_teacher: 0.994
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 0
optim:
  epochs: 50
  weight_decay: 0.04
  weight_decay_end: 0.2
  base_lr: 0.001  # learning rate for a batch size of 1024
  lr: 0.  # will be set after applying scaling rule
  warmup_epochs: 5
  min_lr: 1.0e-06
  clip_grad: 3.0
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999

evaluation:
  eval_period_epoch: 1
  nb_knn: [20]
  train_dataset:
    id: geobench.m-eurosat
    split: train
    transform:
    - id: Listify
    - id: ListCenterCrop
      size: 56
    - id: ChnSelect
      idxs: [3,2,1]
  val_dataset:
    id: geobench.m-eurosat
    split: val
    transform:
    - id: Listify
    - id: ListCenterCrop
      size: 56
    - id: ChnSelect
      idxs: [3,2,1]
  dl_cfg:
    batch_size: 140
    num_workers: 4 # needs to be >0
    pin_memory: False